name: Effect Discipline Benchmark

on:
  release:
    types: [published]
  workflow_dispatch:
    inputs:
      budget:
        description: 'Budget limit in USD'
        required: false
        default: '5.00'
      category:
        description: 'Category to run (leave empty for all)'
        required: false
        default: ''
      sample:
        description: 'Number of tasks to sample (leave empty for all)'
        required: false
        default: ''

jobs:
  benchmark:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '8.0.x'

      - name: Restore cached LLM responses
        uses: actions/cache@v4
        with:
          path: ~/.calor/llm-cache
          key: effect-benchmark-${{ hashFiles('tests/Calor.Evaluation/Tasks/task-manifest-effects.json') }}
          restore-keys: |
            effect-benchmark-

      - name: Restore dependencies
        run: dotnet restore

      - name: Build solution
        run: dotnet build --configuration Release --no-restore

      - name: Run Effect Discipline Benchmark
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          BUDGET="${{ github.event.inputs.budget || '5.00' }}"
          CATEGORY="${{ github.event.inputs.category }}"
          SAMPLE="${{ github.event.inputs.sample }}"

          ARGS="--budget $BUDGET --output effect-results.json --verbose"

          if [ -n "$CATEGORY" ]; then
            ARGS="$ARGS --category $CATEGORY"
          fi

          if [ -n "$SAMPLE" ]; then
            ARGS="$ARGS --sample $SAMPLE"
          fi

          dotnet run --project tests/Calor.Evaluation -- effect-discipline \
            --manifest tests/Calor.Evaluation/Tasks/task-manifest-effects.json \
            $ARGS

      - name: Upload results artifact
        uses: actions/upload-artifact@v4
        with:
          name: effect-benchmark-results
          path: effect-results.json
          retention-days: 90

      - name: Generate summary
        run: |
          echo "## Effect Discipline Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Benchmark completed at $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f effect-results.json ]; then
            # Extract key metrics using jq
            echo "### Summary" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "| Metric | Calor | C# |" >> $GITHUB_STEP_SUMMARY
            echo "|:-------|------:|---:|" >> $GITHUB_STEP_SUMMARY

            CALOR_SCORE=$(jq -r '.summary.averageCalorDisciplineScore // "N/A"' effect-results.json)
            CSHARP_SCORE=$(jq -r '.summary.averageCSharpDisciplineScore // "N/A"' effect-results.json)
            echo "| Discipline Score | $CALOR_SCORE | $CSHARP_SCORE |" >> $GITHUB_STEP_SUMMARY

            CALOR_BUG=$(jq -r '.summary.calorBugPreventionRate // "N/A"' effect-results.json)
            CSHARP_BUG=$(jq -r '.summary.cSharpBugPreventionRate // "N/A"' effect-results.json)
            echo "| Bug Prevention | $CALOR_BUG | $CSHARP_BUG |" >> $GITHUB_STEP_SUMMARY

            RATIO=$(jq -r '.summary.disciplineAdvantageRatio // "N/A"' effect-results.json)
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Advantage Ratio:** ${RATIO}x" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Update website data (on release)
        if: github.event_name == 'release'
        run: |
          # Merge effect discipline results into benchmark-results.json
          if [ -f effect-results.json ] && [ -f website/public/data/benchmark-results.json ]; then
            RATIO=$(jq -r '.summary.disciplineAdvantageRatio // 1' effect-results.json)
            TASK_COUNT=$(jq -r '.summary.totalTasks // 0' effect-results.json)

            # Update benchmark-results.json with EffectDiscipline metric
            jq --arg ratio "$RATIO" --arg count "$TASK_COUNT" '
              .metrics.EffectDiscipline = {
                ratio: ($ratio | tonumber),
                winner: (if ($ratio | tonumber) > 1 then "calor" elif ($ratio | tonumber) < 1 then "csharp" else "tie" end),
                source: "effect-discipline-benchmark",
                taskCount: ($count | tonumber)
              }
            ' website/public/data/benchmark-results.json > tmp.json && mv tmp.json website/public/data/benchmark-results.json

            echo "Updated benchmark-results.json with EffectDiscipline metric"
          fi

      - name: Create PR with updated results (on release)
        if: github.event_name == 'release'
        uses: peter-evans/create-pull-request@v5
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: "chore: update effect discipline benchmark results"
          title: "Update Effect Discipline Benchmark Results"
          body: |
            This PR updates the effect discipline benchmark results from release ${{ github.event.release.tag_name }}.

            ## Results Summary
            See the [workflow run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}) for details.
          branch: benchmark-update-${{ github.run_number }}
          base: main

  compare-baseline:
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.event_name == 'release'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download current results
        uses: actions/download-artifact@v4
        with:
          name: effect-benchmark-results

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Compare against baseline
        run: |
          if [ -f scripts/compare-benchmarks.py ]; then
            python scripts/compare-benchmarks.py \
              --baseline website/public/data/benchmark-results.json \
              --current effect-results.json \
              --metric EffectDiscipline \
              --fail-on-regression 0.05
          else
            echo "Comparison script not found, skipping baseline comparison"
          fi
