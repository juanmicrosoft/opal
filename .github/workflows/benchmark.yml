name: Benchmarks

on:
  # Run weekly on Sunday at midnight UTC
  schedule:
    - cron: '0 0 * * 0'
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      statistical_runs:
        description: 'Number of statistical runs (default 30)'
        required: false
        default: '30'
      llm_eval:
        description: 'Run LLM evaluation'
        required: false
        default: 'false'
        type: boolean
  # Run on pushes to benchmark-related files
  push:
    branches: [main]
    paths:
      - 'tests/Calor.Evaluation/**'
      - 'tests/TestData/Benchmarks/**'
      - '.github/workflows/benchmark.yml'

jobs:
  benchmark:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '8.0.x'

      - name: Restore dependencies
        run: dotnet restore

      - name: Build
        run: dotnet build -c Release --no-restore

      - name: Run benchmarks
        run: |
          dotnet run --project tests/Calor.Evaluation -c Release -- run \
            --format website \
            --output website/public/data/benchmark-results.json \
            --verbose

      - name: Run statistical analysis
        if: github.event_name == 'schedule' || github.event.inputs.statistical_runs != ''
        run: |
          RUNS="${{ github.event.inputs.statistical_runs || '30' }}"
          dotnet run --project tests/Calor.Evaluation -c Release -- run \
            --statistical \
            --runs "$RUNS" \
            --format website \
            --output website/public/data/benchmark-results-statistical.json \
            --verbose

      - name: Generate HTML dashboard
        run: |
          dotnet run --project tests/Calor.Evaluation -c Release -- run \
            --format html \
            --output website/public/data/dashboard

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Generate docs/benchmarking/results.md
        run: node scripts/generate-results-md.js

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            website/public/data/benchmark-results.json
            website/public/data/benchmark-results-statistical.json
            website/public/data/dashboard.html
          retention-days: 90

      - name: Commit results to repo
        if: github.ref == 'refs/heads/main'
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add website/public/data/benchmark-results*.json website/public/data/dashboard.html docs/benchmarking/results.md || true
          git diff --staged --quiet || git commit -m "chore: Update benchmark results [skip ci]"
          git push || echo "No changes to push"

  # LLM evaluation runs separately (more expensive, weekly only or on-demand)
  llm-evaluation:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event.inputs.llm_eval == 'true'
    needs: benchmark
    permissions:
      contents: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '8.0.x'

      - name: Restore dependencies
        run: dotnet restore

      - name: Build
        run: dotnet build -c Release --no-restore

      - name: Ensure output directory exists
        run: mkdir -p website/public/data

      - name: Restore LLM response cache
        uses: actions/cache@v4
        with:
          path: /tmp/calor-llm-cache
          key: llm-cache-opus-${{ github.run_id }}
          restore-keys: |
            llm-cache-opus-

      - name: Run LLM evaluation (Claude Opus)
        id: llm-opus
        continue-on-error: true
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          dotnet run --project tests/Calor.Evaluation -c Release -- llm-tasks \
            --provider claude \
            --model claude-opus-4-5-20251101 \
            --budget 10.00 \
            --output website/public/data/llm-eval-claude-opus.json \
            --verbose

      - name: Check LLM evaluation status
        run: |
          if [ "${{ steps.llm-opus.outcome }}" == "failure" ]; then
            echo "::warning::LLM evaluation failed. Check if ANTHROPIC_API_KEY secret is configured."
          fi
          # List any results that were generated
          ls -la website/public/data/llm-eval-*.json 2>/dev/null || echo "No LLM evaluation results generated"

      - name: Upload LLM evaluation results
        if: hashFiles('website/public/data/llm-eval-*.json') != ''
        uses: actions/upload-artifact@v4
        with:
          name: llm-eval-results
          path: |
            website/public/data/llm-eval-*.json
          retention-days: 90

      - name: Commit LLM results to repo
        if: github.ref == 'refs/heads/main' && hashFiles('website/public/data/llm-eval-*.json') != ''
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git pull --rebase origin main || true
          git add website/public/data/llm-eval-*.json || true
          git diff --staged --quiet || git commit -m "chore: Update LLM evaluation results [skip ci]"
          git push || echo "No changes to push"

  regression-check:
    runs-on: ubuntu-latest
    if: github.event_name == 'push'
    needs: benchmark

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 2

      - name: Download current results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results
          path: current-results

      - name: Check for regression
        run: |
          echo "Checking for benchmark regressions..."
          # TODO: Implement regression detection
          # Compare current results with previous results
          # Alert if any metric drops by more than 10%
          if [ -f "current-results/benchmark-results.json" ]; then
            echo "Benchmark results generated successfully"
            cat current-results/benchmark-results.json | head -50
          else
            echo "Warning: No benchmark results found"
          fi
