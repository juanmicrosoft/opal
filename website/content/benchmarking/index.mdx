---
title: "Benchmarking"
section: "benchmarking"
order: 0
hasChildren: true
---


Calor is evaluated against C# across 13 metrics designed to measure what matters for AI coding agents.

---

## The Metrics

| Category | What It Measures | Why It Matters |
|:---------|:-----------------|:---------------|
| [**Comprehension**](/benchmarking/metrics/comprehension/) | Structural clarity, semantic extractability | Can agents understand code without deep analysis? |
| [**Error Detection**](/benchmarking/metrics/error-detection/) | Bug identification, contract violation detection | Can agents find issues using explicit semantics? |
| [**Edit Precision**](/benchmarking/metrics/edit-precision/) | Targeting accuracy, change isolation | Can agents make precise edits using unique IDs? |
| [**Generation Accuracy**](/benchmarking/metrics/generation-accuracy/) | Compilation success, structural correctness | Can agents produce valid code? |
| [**Task Completion**](/benchmarking/metrics/task-completion/) | LLM code generation success | Can AI complete tasks with Calor vs C#? |
| [**Safety**](/benchmarking/metrics/safety/) | Contract enforcement effectiveness | Does code catch bugs with informative errors? |
| [**Effect Discipline**](/benchmarking/metrics/effect-discipline/) | Side effect management, bug prevention | Does code prevent flaky tests, security violations? |
| [**Token Economics**](/benchmarking/metrics/token-economics/) | Tokens required to represent logic | How much context window does code consume? |
| [**Information Density**](/benchmarking/metrics/information-density/) | Semantic elements per token | How much meaning per token? |

### Calor-Only Metrics

These metrics measure capabilities unique to Calor that have no C# equivalent:

| Category | What It Measures | Why It Matters |
|:---------|:-----------------|:---------------|
| [**Contract Verification**](/benchmarking/metrics/contract-verification/) | Z3 static contract verification rates | Are preconditions/postconditions provably correct? |
| [**Effect Soundness**](/benchmarking/metrics/effect-soundness/) | Effect declarations match actual behavior | Do declared effects accurately describe side effects? |
| [**Interop Coverage**](/benchmarking/metrics/interop-effect-coverage/) | BCL methods covered by effect manifests | How well are .NET interop effects documented? |

---

## Summary Results

<BenchmarkSummaryTable />

**Pattern:** Calor wins on comprehension and precision metrics. C# wins on efficiency metrics.

---

## Key Insight

Calor excels where explicitness matters:
- **Comprehension** - Explicit structure aids understanding
- **Error Detection** - Contracts surface invariant violations
- **Edit Precision** - Unique IDs enable targeted changes
- **Task Completion** - AI completes 23% more tasks successfully with Calor
- **Safety** - Contracts catch 14% more bugs with better error messages
- **Effect Discipline** - Effect system prevents 30% more real-world side effect bugs

C# wins on token efficiency:
- **Token Economics** - Calor's explicit syntax uses more tokens
- **Information Density** - C# packs more per token

This reflects a fundamental tradeoff: **explicit semantics require more tokens but enable better agent reasoning and task completion**.

---

## Learn More

- [Methodology](/benchmarking/methodology/) - How benchmarks work
- [Results](/benchmarking/results/) - Detailed results table
- [Individual Metrics](/benchmarking/metrics/comprehension/) - Deep dive into each metric
