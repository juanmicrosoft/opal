---
title: "Benchmarking"
section: "benchmarking"
order: 0
hasChildren: true
---


Calor is evaluated against C# across 11 metrics designed to measure what matters for AI coding agents.

---

## The Metrics

### Static Analysis Metrics

| Category | What It Measures | Why It Matters |
|:---------|:-----------------|:---------------|
| [**Comprehension**](/benchmarking/metrics/comprehension/) | Structural clarity, semantic extractability | Can agents understand code without deep analysis? |
| [**Error Detection**](/benchmarking/metrics/error-detection/) | Bug identification, contract violation detection | Can agents find issues using explicit semantics? |
| [**Edit Precision**](/benchmarking/metrics/edit-precision/) | Targeting accuracy, change isolation | Can agents make precise edits using unique IDs? |
| [**Generation Accuracy**](/benchmarking/metrics/generation-accuracy/) | Compilation success, structural correctness | Can agents produce valid code? |
| [**Token Economics**](/benchmarking/metrics/token-economics/) | Tokens required to represent logic | How much context window does code consume? |
| [**Information Density**](/benchmarking/metrics/information-density/) | Semantic elements per token | How much meaning per token? |
| [**Refactoring Stability**](/benchmarking/metrics/refactoring-stability/) | ID-based reference preservation | Do unique IDs survive code transformations? |

### LLM-Based Metrics

These metrics use actual LLM code generation to measure real-world effectiveness:

| Category | What It Measures | Why It Matters |
|:---------|:-----------------|:---------------|
| [**Task Completion**](/benchmarking/metrics/task-completion/) | LLM code generation success | Can AI complete tasks with Calor vs C#? |
| [**Safety**](/benchmarking/metrics/safety/) | Contract enforcement effectiveness | Does code catch bugs with informative errors? |
| [**Effect Discipline**](/benchmarking/metrics/effect-discipline/) | Side effect management, bug prevention | Does code prevent flaky tests, security violations? |
| [**Correctness**](/benchmarking/metrics/correctness/) | Edge case handling, bug prevention | Does code produce correct results for edge cases? |

---

## Summary Results

<BenchmarkSummaryTable />

**Pattern:** Calor wins on comprehension and precision metrics. C# wins on efficiency metrics.

---

## Key Insight

Calor excels where explicitness matters:
- **Comprehension (1.51x)** - Explicit structure aids understanding
- **Error Detection (1.22x)** - Contracts surface invariant violations
- **Edit Precision (1.37x)** - Unique IDs enable targeted changes
- **Refactoring Stability (1.36x)** - ID-based references survive transformations
- **Safety (1.59x)** - Contracts catch more bugs with better error messages
- **Effect Discipline (1.11x)** - Effect system prevents real-world side effect bugs

Calor and C# are equivalent on:
- **Correctness (1.0x)** - Both languages achieve 100% on edge case handling when benchmarked fairly

C# wins on efficiency:
- **Token Economics (0.92x)** - Calor's explicit syntax uses more tokens
- **Generation Accuracy (0.97x)** - C# has broader training data
- **Task Completion (0.95x)** - C# syntax familiarity helps LLMs

This reflects a fundamental tradeoff: **explicit semantics require more tokens but enable better agent reasoning and safer code**.

---

## Agent Task Benchmark

The [Agent Task Benchmark](/benchmarking/agent-tasks/) tests Claude's ability to generate correct Calor code from natural language prompts. It validates skill documentation quality by measuring how well Claude can learn Calor syntax.

**Current Pass Rate: 86%** across 89 tasks in 17 categories.

---

## Learn More

- [Methodology](/benchmarking/methodology/) - How benchmarks work
- [Results](/benchmarking/results/) - Detailed results table
- [Agent Tasks](/benchmarking/agent-tasks/) - Claude code generation benchmark
- [Individual Metrics](/benchmarking/metrics/comprehension/) - Deep dive into each metric
