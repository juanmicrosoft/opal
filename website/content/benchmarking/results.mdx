---
title: "Results"
section: "benchmarking"
order: 2
---

# Benchmark Results

<BenchmarkDashboard />

---

## The Tradeoff

The benchmark results reveal a fundamental tradeoff in language design for AI agents:

**Explicitness vs. Efficiency**

Calor's design prioritizes explicit semantics—contracts, effect annotations, unique IDs—that enable better reasoning about program invariants. This comes at the cost of token efficiency, as explicit syntax requires more tokens than implicit conventions.

C#'s ecosystem maturity gives it advantages in areas like task completion and generation accuracy, where LLMs benefit from extensive training data. However, Calor's explicit contracts provide measurable benefits for error detection.

---

## When to Use Calor

Based on results, Calor is most valuable when:

- **Contract verification matters** — The error detection advantage validates explicit contracts
- **Edit precision is important** — Unique IDs enable targeted modifications
- **Agent comprehension is critical** — Explicit structure provides clear signals
- **Token budget is flexible** — You can afford the overhead of explicitness

Use C# when:

- **Token efficiency is paramount** — Context window is limited
- **Ecosystem libraries are needed** — Leverage existing tooling
- **Human readability is priority** — Familiar syntax for human developers

---

## Methodology

All benchmarks are automated and reproducible. Each program is evaluated across 8 metrics comparing Calor and C# implementations.

The evaluation framework:
1. Generates prompts for each program/metric combination
2. Records LLM responses for both Calor and C# versions
3. Evaluates responses against ground truth
4. Computes ratios (greater than 1.0 favors Calor, less than 1.0 favors C#)

See [Methodology](/docs/benchmarking/methodology/) for full details on the evaluation framework.

---

## Running Benchmarks

To regenerate benchmark results:

```bash
# Run the evaluation framework
dotnet run --project tests/Calor.Evaluation -- run -f website -o website/public/data/benchmark-results.json

# Start the website to view results
cd website && npm run dev
```

Results are automatically loaded from `benchmark-results.json` and displayed in the dashboard above.
